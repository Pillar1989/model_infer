# 7.1 Example: Pedestrian Analysis Pipeline

**Mode**: Serial (Det → Pose → ReID)

---

## Overview

```
┌─────────────────────────────────────────────────────────────────┐
│  Pedestrian Analysis Pipeline                                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Input: Video frame                                             │
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │ Model 1: Person Detection (YOLO)                        │    │
│  │   - Detect all persons in frame                         │    │
│  │   - Output: bounding boxes + confidence                 │    │
│  └─────────────────────────────────────────────────────────┘    │
│                        │                                        │
│                        ▼ N detections                           │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │ Model 2: Pose Estimation                                │    │
│  │   - Estimate keypoints for each person                  │    │
│  │   - Input: cropped ROIs (batch)                         │    │
│  │   - Output: 17 keypoints per person                     │    │
│  └─────────────────────────────────────────────────────────┘    │
│                        │                                        │
│                        ▼ N keypoint sets                        │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │ Model 3: ReID Feature Extraction                        │    │
│  │   - Extract appearance features                         │    │
│  │   - Input: cropped ROIs (batch)                         │    │
│  │   - Output: 512-dim feature vector per person           │    │
│  └─────────────────────────────────────────────────────────┘    │
│                        │                                        │
│                        ▼                                        │
│  Output: [{bbox, keypoints, feature}, ...]                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Model Specifications

| Model | Input Size | Output | Purpose |
|-------|------------|--------|---------|
| YOLO11n | 640×640 | boxes [84,8400] | Person detection |
| HRNet-W32 | 256×192 | heatmaps [17,64,48] | Pose estimation |
| OSNet x0.25 | 256×128 | features [512] | Appearance feature |

---

## Data Flow

### Stage 1: Detection

```
┌─────────────────────────────────────────────────────────────────┐
│  Detection Stage                                                │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Preprocess:                                                    │
│    Input:  ctx.image (original frame, e.g., 1920×1080)          │
│    Process:                                                     │
│      - Letterbox resize to 640×640 (preserve aspect ratio)      │
│      - Convert BGR → RGB                                        │
│      - Normalize to [0,1] or with mean/std                      │
│      - Transpose to NCHW                                        │
│    Output: tensor [1, 3, 640, 640]                              │
│                                                                 │
│  Inference:                                                     │
│    YOLO model → [1, 84, 8400]                                   │
│                                                                 │
│  Postprocess:                                                   │
│    - Transpose to [8400, 84]                                    │
│    - Extract boxes (x,y,w,h) and class scores                   │
│    - Filter by confidence threshold (0.25)                      │
│    - Filter class=0 (person only)                               │
│    - Apply NMS (IoU=0.45)                                       │
│    - Scale boxes back to original image coordinates             │
│    Output: [{bbox, score}, ...]                                 │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Stage 2: Pose Estimation

```
┌─────────────────────────────────────────────────────────────────┐
│  Pose Stage                                                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Preprocess:                                                    │
│    Input:  ctx.image + ctx.results.det (N detections)           │
│    Skip:   if N == 0, return nil                                │
│    Process (for each detection):                                │
│      - Expand bbox by 20% for context                           │
│      - Crop ROI from original image                             │
│      - Resize to 256×192                                        │
│      - Normalize                                                │
│      - Stack all crops                                          │
│    Output: tensor [N, 3, 256, 192]                              │
│                                                                 │
│  Inference:                                                     │
│    Pose model → [N, 17, 64, 48] (heatmaps)                      │
│                                                                 │
│  Postprocess:                                                   │
│    - Find argmax in each heatmap                                │
│    - Convert heatmap coords to image coords                     │
│    - Apply sub-pixel refinement (optional)                      │
│    - Map back to original image coordinates                     │
│    Output: [[17 keypoints], ...] for N persons                  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Stage 3: ReID Feature Extraction

```
┌─────────────────────────────────────────────────────────────────┐
│  ReID Stage                                                     │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Preprocess:                                                    │
│    Input:  ctx.image + ctx.results.det (N detections)           │
│    Skip:   if N == 0, return nil                                │
│    Process (for each detection):                                │
│      - Crop ROI from original image (exact bbox)                │
│      - Resize to 256×128                                        │
│      - Normalize with ImageNet mean/std                         │
│      - Stack all crops                                          │
│    Output: tensor [N, 3, 256, 128]                              │
│                                                                 │
│  Inference:                                                     │
│    ReID model → [N, 512]                                        │
│                                                                 │
│  Postprocess:                                                   │
│    - L2 normalize each feature vector                           │
│    - Convert to list of 512-dim vectors                         │
│    Output: [[512-dim feature], ...] for N persons               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Timing Analysis

### Per-Stage Timing

```
┌─────────────────────────────────────────────────────────────────┐
│  Timing Breakdown (N=5 persons)                                 │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Stage 1: Detection                                             │
│    Preprocess:   10 ms  (resize, normalize)                     │
│    Inference:    15 ms  (NPU)                                   │
│    Postprocess:   3 ms  (NMS, filtering)                        │
│    Subtotal:     28 ms                                          │
│                                                                 │
│  Stage 2: Pose (batch=5)                                        │
│    Preprocess:    5 ms  (5× crop + resize)                      │
│    Inference:    20 ms  (NPU, batch=5)                          │
│    Postprocess:   2 ms  (heatmap argmax)                        │
│    Subtotal:     27 ms                                          │
│                                                                 │
│  Stage 3: ReID (batch=5)                                        │
│    Preprocess:    4 ms  (5× crop + resize)                      │
│    Inference:    15 ms  (NPU, batch=5)                          │
│    Postprocess:   1 ms  (L2 normalize)                          │
│    Subtotal:     20 ms                                          │
│                                                                 │
│  Total:          75 ms → ~13 FPS                                │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Scaling with Detection Count

| Persons (N) | Det | Pose | ReID | Total | FPS |
|-------------|-----|------|------|-------|-----|
| 0 | 28ms | skip | skip | 28ms | 36 |
| 1 | 28ms | 15ms | 12ms | 55ms | 18 |
| 5 | 28ms | 27ms | 20ms | 75ms | 13 |
| 10 | 28ms | 40ms | 30ms | 98ms | 10 |
| 20 | 28ms | 65ms | 50ms | 143ms | 7 |

---

## Output Format

### Final Result Structure

```
{
    frame_id: 123,

    det: [
        { bbox: [x1,y1,x2,y2], score: 0.92 },
        { bbox: [x1,y1,x2,y2], score: 0.87 },
        ...
    ],

    pose: [
        [ {x,y,conf}, {x,y,conf}, ... ],  -- 17 keypoints for person 0
        [ {x,y,conf}, {x,y,conf}, ... ],  -- 17 keypoints for person 1
        ...
    ],

    reid: [
        [0.12, -0.34, 0.56, ...],  -- 512-dim feature for person 0
        [0.23, -0.45, 0.67, ...],  -- 512-dim feature for person 1
        ...
    ]
}
```

---

## Use Cases

### Tracking Integration

```
┌─────────────────────────────────────────────────────────────────┐
│  Tracking with Pipeline Output                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Frame N:                                                       │
│    pipeline.run(frame) → results                                │
│                                                                 │
│  For each person:                                               │
│    - Use reid feature for appearance matching                   │
│    - Use bbox IoU for spatial matching                          │
│    - Update track state                                         │
│                                                                 │
│  Track output:                                                  │
│    { track_id, bbox, keypoints, feature, age, state }           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

### Action Recognition

```
┌─────────────────────────────────────────────────────────────────┐
│  Action Recognition Extension                                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Collect pose history per track:                                │
│    track_1: [pose_t-15, pose_t-14, ..., pose_t]                 │
│                                                                 │
│  Feed to action recognition model:                              │
│    action_model(pose_sequence) → action_class                   │
│                                                                 │
│  Note: Action model would be a separate pipeline                │
│        or 4th model (requires decomposition strategy)           │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Memory Usage

```
┌─────────────────────────────────────────────────────────────────┐
│  Memory Budget                                                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  Model weights:                                                 │
│    YOLO11n:    ~10 MB                                           │
│    HRNet:      ~50 MB                                           │
│    OSNet:      ~10 MB                                           │
│    Total:      ~70 MB                                           │
│                                                                 │
│  Runtime buffers (N=10):                                        │
│    Det input:   1.2 MB                                          │
│    Det output:  2.8 MB                                          │
│    Pose input:  10× 0.4 MB = 4 MB                               │
│    Pose output: 10× 0.2 MB = 2 MB                               │
│    ReID input:  10× 0.4 MB = 4 MB                               │
│    ReID output: 10× 2 KB = 0.02 MB                              │
│    Total:       ~14 MB                                          │
│                                                                 │
│  Grand total:   ~84 MB                                          │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```
